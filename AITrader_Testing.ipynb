{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae854e-309f-462f-b1cf-f89a255c8b2f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from AITrader_Funcs import *\n",
    "\n",
    "WINDOW_SIZE = 20\n",
    "DATA_SAVE_DIRECTORY = \"Data\"\n",
    "MODEL_LOGS_PATH = os.path.join(\"Training\", \"Logs\")\n",
    "MODEL_SAVE_PATH = os.path.join(\"Training\", \"Saved_Models\", \"DQN_Model\")\n",
    "\n",
    "# Get data\n",
    "training_data = get_data(start_date = datetime(2018, 1, 1), end_date = datetime(2019, 1, 1), interval = '1d', output_type = 2, window_sample = WINDOW_SIZE)\n",
    "time.sleep(1)\n",
    "sample_data = get_data(start_date = datetime(2019,1,1), end_date = datetime(2021, 1, 1), interval = '1d', output_type = 2, window_sample = 0)\n",
    "sample_data = pad_dummy_day(sample_data) # Do twice to show position and trade in plotting\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "if not os.path.exists(DATA_SAVE_DIRECTORY):\n",
    "    os.makedirs(DATA_SAVE_DIRECTORY)\n",
    "\n",
    "# Save data\n",
    "training_data.to_csv(f\"{DATA_SAVE_DIRECTORY}/training_data.csv\")\n",
    "sample_data.to_csv(f\"{DATA_SAVE_DIRECTORY}/sample_data.csv\")\n",
    "\n",
    "# Make env\n",
    "training_env = MyStocksEnv(df=training_data, window_size=WINDOW_SIZE, frame_bound=(WINDOW_SIZE, len(training_data)))\n",
    "sample_env = MyStocksEnv(df=sample_data, window_size=WINDOW_SIZE, frame_bound=(WINDOW_SIZE, len(sample_data)))\n",
    "\n",
    "def Make_Model (logs_path): \n",
    "    model = DQN(\"MlpPolicy\", training_env, verbose=0, tensorboard_log = logs_path, batch_size = 32, buffer_size = 20000, gamma = 0.99, learning_rate = 0.001, exploration_initial_eps=1.0, exploration_fraction = 0.70, exploration_final_eps=0.05, seed = 29)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fca1e1-2a27-4165-97be-9068bd7d3d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- TRAIN AND TEST IN SAMPLE ---------------------\n",
    "\n",
    "# Get initial observation\n",
    "done = False\n",
    "tick = 0\n",
    "action_list = []\n",
    "retrain_period = 60\n",
    "training_data_size = 90\n",
    "obs, info = sample_env.reset()\n",
    "\n",
    "# Predict action for each timestep in data\n",
    "for tick in trange(0, len(sample_data)):\n",
    "\n",
    "    tick_window_adjusted = tick - WINDOW_SIZE\n",
    "\n",
    "    # Train intervals after WINDOW_SIZE and once on start\n",
    "    if (tick_window_adjusted > 0 and tick_window_adjusted % retrain_period == 0) or tick == 0:\n",
    "\n",
    "        if tick == 0:\n",
    "            # Get initial data\n",
    "            training_data = pd.concat([training_data.iloc[-WINDOW_SIZE:].reset_index(drop=True), sample_data.iloc[WINDOW_SIZE: (WINDOW_SIZE +training_data_size)]], ignore_index = True)\n",
    "            training_env = MyStocksEnv(df=training_data, window_size=WINDOW_SIZE, frame_bound=(WINDOW_SIZE, len(training_data)))\n",
    "\n",
    "        # Update environment if past tick 0\n",
    "        if tick_window_adjusted > 0:\n",
    "            training_data = sample_data.iloc[(tick - WINDOW_SIZE):min((tick + training_data_size), len(sample_data))].reset_index(drop=True)\n",
    "            # Remake env\n",
    "            training_env = MyStocksEnv(df=training_data, window_size=WINDOW_SIZE, frame_bound=(WINDOW_SIZE, len(training_data)))\n",
    "\n",
    "        # Train model\n",
    "        model = Make_Model(MODEL_LOGS_PATH)\n",
    "        model_unique_day_path = f'{MODEL_SAVE_PATH}_{tick}_days'\n",
    "        train_model(model, training_env, training_timesteps = 30000, model_save_path = model_unique_day_path, n_evaluations = 30, reward_threshold = 10, verbose = 0)\n",
    "        model = DQN.load(os.path.join(model_unique_day_path, 'best_model.zip'))\n",
    "\n",
    "    tick += 1\n",
    "    \n",
    "    # Make prediction, next step\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = sample_env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    action_list.append(action)\n",
    "\n",
    "    if done:\n",
    "        # Get the action of the last index (current day)\n",
    "        obs = sample_env._get_observation()\n",
    "        action, states = model.predict(obs, deterministic=True)\n",
    "        break\n",
    "\n",
    "# Plot results\n",
    "plt.cla()\n",
    "sample_env.unwrapped.render_all(action = action)\n",
    "plt.show()\n",
    "# ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d17858f-4e8d-4c1e-b19a-0b0e8294603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- TRAIN AND TEST OUT OF SAMPLE ---------------------\n",
    "\n",
    "# Initialise variables\n",
    "done = False\n",
    "tick = 0\n",
    "action_list = []\n",
    "retrain_period = 60\n",
    "training_data_size = 90\n",
    "obs, info = sample_env.reset()\n",
    "\n",
    "# Predict action for each timestep in data\n",
    "for tick in trange(0, len(sample_data)):\n",
    "\n",
    "    tick_window_adjusted = tick - WINDOW_SIZE\n",
    "\n",
    "    # Train intervals after WINDOW_SIZE and once on start\n",
    "    if (tick_window_adjusted > 0 and tick_window_adjusted % retrain_period == 0) or tick == 0:\n",
    "\n",
    "        if tick == 0:\n",
    "            # Get data\n",
    "            training_data = training_data.iloc[-(WINDOW_SIZE + training_data_size):].reset_index(drop=True)\n",
    "            training_env = MyStocksEnv(df=training_data, window_size=WINDOW_SIZE, frame_bound=(WINDOW_SIZE, len(training_data)))\n",
    "    \n",
    "        # Update environment if past tick 0\n",
    "        if tick_window_adjusted > 0:\n",
    "            # Add days to training data\n",
    "            training_data = pd.concat([training_data, sample_data.iloc[tick_window_adjusted -retrain_period : tick_window_adjusted]], ignore_index=True)\n",
    "            # Drop oldest days\n",
    "            training_data = training_data.drop(training_data.index[:retrain_period]).copy()\n",
    "            # Remake env\n",
    "            training_env = MyStocksEnv(df=training_data, window_size=WINDOW_SIZE, frame_bound=(WINDOW_SIZE, len(training_data)))\n",
    "\n",
    "        # Train model\n",
    "        model = Make_Model(MODEL_LOGS_PATH)\n",
    "        model_unique_day_path = f'{MODEL_SAVE_PATH}_{tick}_days'\n",
    "        train_model(model, training_env, training_timesteps = 30000, model_save_path = model_unique_day_path, n_evaluations = 30, reward_threshold = 10, verbose = 0)\n",
    "        model = DQN.load(os.path.join(model_unique_day_path, 'best_model.zip'))\n",
    "\n",
    "    tick += 1\n",
    "    \n",
    "    # Make prediction, next step\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = sample_env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    action_list.append(action)\n",
    "\n",
    "    if done:\n",
    "        # Get the action of the last index (current day)\n",
    "        obs = sample_env._get_observation()\n",
    "        action, states = model.predict(obs, deterministic=True)\n",
    "        break\n",
    "\n",
    "# Plot results\n",
    "plt.cla()\n",
    "sample_env.unwrapped.render_all(action = action)\n",
    "plt.show()\n",
    "# ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d0e686-d897-406b-a053-9287f9e0dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ BACKTESTING PLOT -----------------------\n",
    "from backtesting import Backtest, Strategy\n",
    "import math\n",
    "\n",
    "# Read data\n",
    "sample_data = pd.read_csv(\"Data/sample_data.csv\", index_col=0)\n",
    "sample_data.index = pd.to_datetime(sample_data.index, utc = True)\n",
    "sample_env.reset()\n",
    "\n",
    "# Load initial model\n",
    "model_unique_day_path = f'{MODEL_SAVE_PATH}_0_days'\n",
    "model = DQN.load(os.path.join(model_unique_day_path, 'best_model.zip'))\n",
    "\n",
    "class DQN_Strategy(Strategy):\n",
    "    def init(self):\n",
    "        self.env = sample_env\n",
    "        self.obs = self.env.reset()[0]\n",
    "        self.model = model\n",
    "        self.entry_price = None  # Track entry price of the current position\n",
    "        self.tick = 0\n",
    "        self.last_trade_action = None\n",
    "        self.lockout = False\n",
    "        \n",
    "    def next(self):\n",
    "\n",
    "        self.tick += 1\n",
    "        tick_window_adjusted = self.tick - WINDOW_SIZE\n",
    "        current_price = self.data.Close[-1]\n",
    "\n",
    "        # Investing / snowballing\n",
    "        size = math.floor(self.equity / current_price)\n",
    "\n",
    "        # Skip the early window data for sampling\n",
    "        if len(self.data) <= self.env.window_size +1:\n",
    "            return\n",
    "        \n",
    "        # If done close any open positions and return\n",
    "        if self.tick == len(sample_data) -2:\n",
    "            print(self.tick)\n",
    "            if self.position:\n",
    "                self.position.close() \n",
    "            return \n",
    "        \n",
    "        action = action_list[tick_window_adjusted-1]\n",
    "\n",
    "        # ----------- Lockout Trades -----------------\n",
    "        # Skip if currently locked out and action is same as last trade\n",
    "        if self.lockout and action == self.last_trade_action:\n",
    "            return\n",
    "\n",
    "        # If action is opposite of last trade, reset lockout\n",
    "        if self.lockout and action != self.last_trade_action:\n",
    "            self.lockout = False\n",
    "\n",
    "        # Stop loss and lockout at -10% loss\n",
    "        if self.position.is_long:\n",
    "            perc_return = (current_price - self.entry_price) / self.entry_price\n",
    "            if perc_return < -0.1:\n",
    "                self.position.close()\n",
    "                self.lockout = True\n",
    "                self.entry_price = None\n",
    "        if self.position.is_short:\n",
    "            perc_return = (self.entry_price - current_price) / self.entry_price\n",
    "            if perc_return < -0.1:\n",
    "                self.position.close()\n",
    "                self.lockout = True\n",
    "                self.entry_price = None\n",
    "        # ------------------------------------------------\n",
    "\n",
    "        # Sell action\n",
    "        if action == 0 and not self.lockout:\n",
    "            if self.position.is_long or not self.position:\n",
    "                self.position.close()\n",
    "                self.sell(size = size)\n",
    "                self.entry_price = current_price  # Set new entry\n",
    "                self.last_trade_action = 0\n",
    "                \n",
    "        # Buy action        \n",
    "        if action == 1 and not self.lockout:\n",
    "            if self.position.is_short or not self.position:\n",
    "                self.position.close()\n",
    "                self.buy(size = size)\n",
    "                self.entry_price = current_price  # Set new entry\n",
    "                self.last_trade_action = 1\n",
    "                \n",
    "def commission_calc(size, price):\n",
    "\n",
    "    # Mimmicking Trading-212 overnight fees and fx fees\n",
    "    commission = abs((size * price * 0.005) * 0.0025) # fx fee\n",
    "\n",
    "    # Trade duration is estimated as backtesting.py has limited data access in default commision function\n",
    "    Average_Trade_Duration = 2\n",
    "    \n",
    "    if size < 0:\n",
    "        commission += abs(0.000139 * (size * price) * Average_Trade_Duration) # overnight sell\n",
    "    else:   \n",
    "        commission += abs(0.00011 * (size * price) * Average_Trade_Duration) # overnight buy\n",
    "\n",
    "    commission += abs((size * price) * 0.0001) # small slippage\n",
    "    \n",
    "    return commission\n",
    "\n",
    "bt = Backtest(sample_data, DQN_Strategy, commission= commission_calc, margin= 1/5 , cash = 100000, trade_on_close = True)\n",
    "stats = bt.run()\n",
    "bt.plot()\n",
    "\n",
    "trades = stats._trades\n",
    "\n",
    "print(trades.tail())\n",
    "print(stats)\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bcc20a-695e-4686-ad53-b4f9978805f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
